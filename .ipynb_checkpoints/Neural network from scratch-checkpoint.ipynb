{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe37a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb54b6",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7240cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f364c2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5e4e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i) # the size of the image\n",
    "    plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90005bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   778  779  780  781  782  783  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_X.reshape([60000,28*28])).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1fdbce",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6b1b69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_X.reshape(60000,784))\n",
    "X_test = scaler.transform(test_X.reshape(10000,784))\n",
    "# standard scaling didnt work well because its not fit to this type of data where\n",
    "# the valus is mostly 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f9a97",
   "metadata": {},
   "source": [
    "## one hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "43b5ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23f36296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5923,)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "train_y = train_y.reshape([60000,1])\n",
    "test_y = test_y.reshape([10000,1])\n",
    "\n",
    "yy0 = train_y[train_y==0]\n",
    "yy1 = train_y[train_y==1]\n",
    "\n",
    "print(yy0.shape)\n",
    "\n",
    "y_train = enc.fit_transform(train_y)\n",
    "y_test = enc.transform(test_y)\n",
    "y_train_np =y_train.toarray()\n",
    "y_test_np =y_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c13fd5",
   "metadata": {},
   "source": [
    "# Creating our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b6e518c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(x):\n",
    "    return 1./(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid_func(x):\n",
    "    return sigmoid_func(x)*(1-sigmoid_func(x))\n",
    "\n",
    "def relu_func(x):\n",
    "    return np.maximum(0.,x)\n",
    "\n",
    "def d_relu_func(x):\n",
    "    return np.greater(x, 0.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111aac5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6ab37aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14656\\296851988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mE_d_w1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta_a2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mw1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mE_d_w1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mw2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mE_d_w2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mw3\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mE_d_w3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_slice = X_train[:10000,:]\n",
    "y_train_slice = y_train_np[:10000,:]\n",
    "a1_out = X_train_slice.reshape(10000, 784)\n",
    "\n",
    "w1 = np.random.random_sample((784,10))\n",
    "bias1 = np.random.random_sample((1,10))\n",
    "\n",
    "w2 = np.random.random_sample((10,10))\n",
    "bias2 = np.random.random_sample((1,10))\n",
    "\n",
    "w3 = np.random.random_sample((10,10))\n",
    "bias3 = np.random.random_sample((1,10))\n",
    "\n",
    "lr = 0.0001\n",
    "E=0\n",
    "epoches=10\n",
    "for epoch in range(epoches):\n",
    "    a2_net = np.matmul(a1_out,w1)+bias1\n",
    "    a2_out = sigmoid_func(a2_net)\n",
    "    a3_net = np.matmul(a2_out,w2)+bias2\n",
    "    a3_out = sigmoid_func(a3_net)\n",
    "    a4_net = np.matmul(a3_out,w3)+bias3\n",
    "    a4_out = sigmoid_func(a4_net)\n",
    "        \n",
    "    delta_a4 = (1/y_train_slice.shape[0])*-(y_train_slice - a4_out)*a4_out*(1-a4_out)\n",
    "    E_d_w3 = np.matmul(a3_out.T,delta_a4)\n",
    "    delta_a3 =np.matmul( np.matmul(delta_a4, w3), (np.matmul((1-a3_out.T), a3_out)))\n",
    "    E_d_w2 = np.matmul(a2_out.T,delta_a3)\n",
    "    delta_a2 =np.matmul( np.matmul(delta_a3, w2), (np.matmul((1-a2_out.T), a2_out)))\n",
    "    E_d_w1 = np.matmul(a1_out.T,delta_a2)\n",
    "\n",
    "    w1 -= lr/m*E_d_w1\n",
    "    w2 -= lr*E_d_w2\n",
    "    w3 -= lr*E_d_w3\n",
    "    bias1-=lr*np.sum(delta_a2)\n",
    "    bias2-=lr*np.sum(delta_a3.T)\n",
    "    bias3-=lr*np.sum(delta_a4)\n",
    "    if epoch==0:\n",
    "        E_total = (-1/y_train_slice.shape[0])*np.sum(y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out))\n",
    "        print(\"the total error before training is: \"+str(E_total))\n",
    "    elif epoch>=(epoches-10):\n",
    "        E_total = (-1/y_train_slice.shape[0])*np.sum(y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out))\n",
    "        E+=E_total\n",
    "E_total = (-1/y_train_slice.shape[0])*np.sum(y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out))\n",
    "print(\"the total error is: \"+str(E/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b354d",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "  0. Write the right shapes of every instance in the `train_model()`function.\n",
    "  1. Make sure the forward/back propagation in the `train_model()`function works appropriately.\n",
    "  2. Make the `train_model()`function to work with batches.\n",
    "  3. Make predict and evaluate functions\n",
    "\n",
    "*  Check out others people work to get insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5297012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "X_train_01 = X_train[(y_train == 0) | (y_train == 1)]\n",
    "y_train_01 = y_train_cat[(y_train == 0) | (y_train == 1)]\n",
    "\n",
    "X_test_01 = X_test[(y_test == 0) | (y_test == 1)]\n",
    "y_test_01 = y_test_cat[(y_test == 0) | (y_test == 1)]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_01, y_train_01, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1c458819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoches):\n",
    "    X_train_slice = X_train_sub[:10000,:]\n",
    "    y_train_slice = y_train_sub[:10000,:]\n",
    "    a1_out = X_train_slice.reshape(10000, 784)\n",
    "\n",
    "    w1 = np.random.uniform(-1,1,(784,10))\n",
    "    bias1 = np.random.uniform(-1,1,(1,10))\n",
    "\n",
    "    w2 = np.random.uniform(-1,1,(10,10))\n",
    "    bias2 = np.random.uniform(-1,1,(1,10))\n",
    "\n",
    "    w3 = np.random.uniform(-1,1,(10,10))\n",
    "    bias3 = np.random.uniform(-1,1,(1,10))\n",
    "\n",
    "    lr = 0.001\n",
    "    E=0\n",
    "    for epoch in range(epoches):\n",
    "        a2_net = np.matmul(a1_out,w1)+bias1\n",
    "        a2_out = sigmoid_func(a2_net)\n",
    "        a3_net = np.matmul(a2_out,w2)+bias2\n",
    "        a3_out = sigmoid_func(a3_net)\n",
    "        a4_net = np.matmul(a3_out,w3)+bias3\n",
    "        a4_out = sigmoid_func(a4_net)\n",
    "        \n",
    "        delta_a4 = -y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out)\n",
    "        E_d_w3 = np.matmul(a3_out.T,delta_a4)\n",
    "        delta_a3 =np.matmul( np.matmul(delta_a4, w3), (np.matmul((1-a3_out.T), a3_out)))\n",
    "        E_d_w2 = np.matmul(a2_out.T,delta_a3)\n",
    "        delta_a2 =np.matmul( np.matmul(delta_a3, w2), (np.matmul((1-a2_out.T), a2_out)))\n",
    "        E_d_w1 = np.matmul(a1_out.T,delta_a2)\n",
    "\n",
    "        w1 -= (1/y_train_slice.shape[0])*lr*E_d_w1\n",
    "        w2 -= (1/y_train_slice.shape[0])*lr*E_d_w2\n",
    "        w3 -= (1/y_train_slice.shape[0])*lr*E_d_w3\n",
    "        bias1-=(1/y_train_slice.shape[0])*lr*np.sum(delta_a2, axis=0).reshape(1,10)\n",
    "        bias2-=(1/y_train_slice.shape[0])*lr*np.sum(delta_a3, axis=0).reshape(1,10)\n",
    "        bias3-=(1/y_train_slice.shape[0])*lr*np.sum(delta_a4, axis=0).reshape(1,10)\n",
    "        if epoch==0:\n",
    "            E_total = (-1/y_train_slice.shape[0])*np.sum(y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out))\n",
    "            print(\"the total error before training is: \"+str(E_total))\n",
    "        elif epoch==epoches-1:\n",
    "            E_total = (-1/y_train_slice.shape[0])*np.sum(y_train_slice*np.log(a4_out)+(1-y_train_slice)*np.log(1-a4_out))\n",
    "            print(\"the total error is: \"+str(E_total))\n",
    "    \n",
    "    print(\"a1_out.shape: \"+str(a1_out.shape))\n",
    "    print(\"a2_net.shape: \"+str(a2_net.shape))\n",
    "    print(\"a2_out.shape: \"+str(a2_out.shape))\n",
    "    print(\"a3_net.shape: \"+str(a3_net.shape))\n",
    "    print(\"a3_out.shape: \"+str(a3_out.shape))\n",
    "    print(\"a4_net.shape: \"+str(a4_net.shape))\n",
    "    print(\"a4_out.shape: \"+str(a4_out.shape)+\"\\n\")\n",
    "\n",
    "    print(\"w1.shape: \"+str(w1.shape))\n",
    "    print(\"w2.shape: \"+str(w2.shape))\n",
    "    print(\"w3.shape: \"+str(w3.shape)+\"\\n\")\n",
    "    \n",
    "    print(\"bias1.shape: \"+str(bias1.shape))\n",
    "    print(\"bias2.shape: \"+str(bias2.shape))\n",
    "    print(\"bias3.shape: \"+str(bias3.shape)+\"\\n\")\n",
    "    \n",
    "    print(\"delta_a4.shape: \"+str(delta_a4.shape))\n",
    "    print(\"delta_a3.shape: \"+str(delta_a3.shape))\n",
    "    print(\"delta_a2.shape: \"+str(delta_a2.shape)+\"\\n\")\n",
    "    \n",
    "    print(\"E_d_w1.shape: \"+str(E_d_w1.shape))\n",
    "    print(\"E_d_w2.shape: \"+str(E_d_w2.shape))\n",
    "    print(\"E_d_w3.shape: \"+str(E_d_w3.shape)+\"\\n\")\n",
    "    \n",
    "    return w1, w2, w3, bias1, bias2, bias3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "50749f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naor\\AppData\\Local\\Temp\\ipykernel_14656\\2833449474.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total error before training is: 10.438756288467225\n",
      "the total error is: 11.691010547252626\n",
      "a1_out.shape: (10000, 784)\n",
      "a2_net.shape: (10000, 10)\n",
      "a2_out.shape: (10000, 10)\n",
      "a3_net.shape: (10000, 10)\n",
      "a3_out.shape: (10000, 10)\n",
      "a4_net.shape: (10000, 10)\n",
      "a4_out.shape: (10000, 10)\n",
      "\n",
      "w1.shape: (784, 10)\n",
      "w2.shape: (10, 10)\n",
      "w3.shape: (10, 10)\n",
      "\n",
      "bias1.shape: (1, 10)\n",
      "bias2.shape: (1, 10)\n",
      "bias3.shape: (1, 10)\n",
      "\n",
      "delta_a4.shape: (10000, 10)\n",
      "delta_a3.shape: (10000, 10)\n",
      "delta_a2.shape: (10000, 10)\n",
      "\n",
      "E_d_w1.shape: (784, 10)\n",
      "E_d_w2.shape: (10, 10)\n",
      "E_d_w3.shape: (10, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3, bias1, bias2, bias3 = train_model(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f29f5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(w1, w2, w3, bias1, bias2, bias3, x):\n",
    "    a1_out = x.reshape(x.shape[0], 784)\n",
    "    a2_net = np.matmul(a1_out,w1)+bias1\n",
    "    a2_out = sigmoid_func(a2_net)\n",
    "    a3_net = np.matmul(a2_out,w2)+bias2\n",
    "    a3_out = sigmoid_func(a3_net)\n",
    "    a4_net = np.matmul(a3_out,w3)+bias3\n",
    "    a4_out = sigmoid_func(a4_net)\n",
    "    return a4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c189b6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float128'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14656\\156936423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#y_train_slice = y_train_np[7,:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#y_train_slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14656\\1149775317.py\u001b[0m in \u001b[0;36mpredict_single\u001b[1;34m(w1, w2, w3, bias1, bias2, bias3, x)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ma1_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ma2_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbias1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ma2_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ma3_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbias2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0ma3_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma3_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14656\\586499335.py\u001b[0m in \u001b[0;36msigmoid_func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0md_sigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[0;32m    314\u001b[0m                                  \"{!r}\".format(__name__, attr))\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float128'"
     ]
    }
   ],
   "source": [
    "x = X_train[1:7,:].reshape(6, 784)\n",
    "predict_single(w1, w2, w3, bias1, bias2, bias3, x)\n",
    "#y_train_slice = y_train_np[7,:]\n",
    "#y_train_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b081c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_np[7,:], y_train_np[24,:], y_train_np[240,:], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0722c06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2, w3, bias1, bias2, bias3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "903305e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00013451, 0.00013451, 0.00013451, 0.00013451, 0.00013451,\n",
       "        0.00013451, 0.00013451, 0.00013451, 0.00013451, 0.00013451]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(delta_a2, axis=0).reshape(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6c3ec7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_slice[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f12f60",
   "metadata": {},
   "source": [
    "# This is very important to mention:\n",
    " high values of weights means overfitting\n",
    " and high values of bias means underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168285da",
   "metadata": {},
   "source": [
    "maybe i have those weird values on the weights and biases because i need to use a \n",
    "multivariable version of the chain rule instead (of course the cost function is different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8014e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98d1821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "My_model = keras.Sequential([\n",
    "    keras.layers.Reshape(target_shape=(28 * 28,)),\n",
    "    keras.layers.Dense(units=10, activation='sigmoid'),\n",
    "    keras.layers.Dense(units=10, activation='sigmoid'),\n",
    "    keras.layers.Dense(units=10, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30cc2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "My_model.compile(optimizer='adam', \n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85eb282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2903 - MSE: 0.2025\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2468 - MSE: 0.1995\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2068 - MSE: 0.1962\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1717 - MSE: 0.1938\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1378 - MSE: 0.1913\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1041 - MSE: 0.1886\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0713 - MSE: 0.1864\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0382 - MSE: 0.1842\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0032 - MSE: 0.1815\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9673 - MSE: 0.1792\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9306 - MSE: 0.1765\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8927 - MSE: 0.1741\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8540 - MSE: 0.1716\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8140 - MSE: 0.1690\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7745 - MSE: 0.1664\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7344 - MSE: 0.1641\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6940 - MSE: 0.1613\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6535 - MSE: 0.1591\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6143 - MSE: 0.1563\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5749 - MSE: 0.1543\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5369 - MSE: 0.1522\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5003 - MSE: 0.1503\n",
      "Epoch 23/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4652 - MSE: 0.1483\n",
      "Epoch 24/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4303 - MSE: 0.1467\n",
      "Epoch 25/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3975 - MSE: 0.1450\n",
      "Epoch 26/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3655 - MSE: 0.1436\n",
      "Epoch 27/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3347 - MSE: 0.1421\n",
      "Epoch 28/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3043 - MSE: 0.1404\n",
      "Epoch 29/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2762 - MSE: 0.1394\n",
      "Epoch 30/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2487 - MSE: 0.1381\n",
      "Epoch 31/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2215 - MSE: 0.1369\n",
      "Epoch 32/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1954 - MSE: 0.1358\n",
      "Epoch 33/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1704 - MSE: 0.1347\n",
      "Epoch 34/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1466 - MSE: 0.1338\n",
      "Epoch 35/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1224 - MSE: 0.1327\n",
      "Epoch 36/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0994 - MSE: 0.1317\n",
      "Epoch 37/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0767 - MSE: 0.1310\n",
      "Epoch 38/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0551 - MSE: 0.1299\n",
      "Epoch 39/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0339 - MSE: 0.1291\n",
      "Epoch 40/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0135 - MSE: 0.1282\n",
      "Epoch 41/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9922 - MSE: 0.1273\n",
      "Epoch 42/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9714 - MSE: 0.1264\n",
      "Epoch 43/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9510 - MSE: 0.1257\n",
      "Epoch 44/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9318 - MSE: 0.1246\n",
      "Epoch 45/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9123 - MSE: 0.1238\n",
      "Epoch 46/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8922 - MSE: 0.1228\n",
      "Epoch 47/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8730 - MSE: 0.1221\n",
      "Epoch 48/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8548 - MSE: 0.1209\n",
      "Epoch 49/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8366 - MSE: 0.1203\n",
      "Epoch 50/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8186 - MSE: 0.1191\n",
      "Epoch 51/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8002 - MSE: 0.1187\n",
      "Epoch 52/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7835 - MSE: 0.1175\n",
      "Epoch 53/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7664 - MSE: 0.1168\n",
      "Epoch 54/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7496 - MSE: 0.1159\n",
      "Epoch 55/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7324 - MSE: 0.1151\n",
      "Epoch 56/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7164 - MSE: 0.1142\n",
      "Epoch 57/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6995 - MSE: 0.1135\n",
      "Epoch 58/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6843 - MSE: 0.1126\n",
      "Epoch 59/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6696 - MSE: 0.1120\n",
      "Epoch 60/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6543 - MSE: 0.1108\n",
      "Epoch 61/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.6397 - MSE: 0.1103\n",
      "Epoch 62/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6251 - MSE: 0.1096\n",
      "Epoch 63/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6120 - MSE: 0.1086\n",
      "Epoch 64/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5982 - MSE: 0.1083\n",
      "Epoch 65/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5846 - MSE: 0.1072\n",
      "Epoch 66/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5706 - MSE: 0.1066\n",
      "Epoch 67/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5582 - MSE: 0.1060\n",
      "Epoch 68/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5468 - MSE: 0.1052\n",
      "Epoch 69/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5349 - MSE: 0.1046\n",
      "Epoch 70/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5231 - MSE: 0.1041\n",
      "Epoch 71/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5110 - MSE: 0.1032\n",
      "Epoch 72/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5000 - MSE: 0.1027\n",
      "Epoch 73/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4890 - MSE: 0.1019\n",
      "Epoch 74/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4786 - MSE: 0.1014\n",
      "Epoch 75/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4695 - MSE: 0.1011\n",
      "Epoch 76/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4589 - MSE: 0.1003\n",
      "Epoch 77/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4491 - MSE: 0.0996\n",
      "Epoch 78/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4399 - MSE: 0.0992\n",
      "Epoch 79/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4308 - MSE: 0.0984\n",
      "Epoch 80/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4210 - MSE: 0.0980\n",
      "Epoch 81/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4129 - MSE: 0.0974\n",
      "Epoch 82/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4045 - MSE: 0.0968\n",
      "Epoch 83/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3958 - MSE: 0.0963\n",
      "Epoch 84/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3878 - MSE: 0.0959\n",
      "Epoch 85/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.3792 - MSE: 0.0954\n",
      "Epoch 86/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3715 - MSE: 0.0947\n",
      "Epoch 87/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3635 - MSE: 0.0943\n",
      "Epoch 88/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3563 - MSE: 0.0938\n",
      "Epoch 89/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3499 - MSE: 0.0930\n",
      "Epoch 90/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3418 - MSE: 0.0930\n",
      "Epoch 91/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3349 - MSE: 0.0921\n",
      "Epoch 92/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3281 - MSE: 0.0919\n",
      "Epoch 93/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3221 - MSE: 0.0914\n",
      "Epoch 94/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3151 - MSE: 0.0910\n",
      "Epoch 95/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3085 - MSE: 0.0906\n",
      "Epoch 96/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3023 - MSE: 0.0901\n",
      "Epoch 97/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2965 - MSE: 0.0894\n",
      "Epoch 98/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2908 - MSE: 0.0893\n",
      "Epoch 99/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2852 - MSE: 0.0891\n",
      "Epoch 100/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2798 - MSE: 0.0883\n",
      "Epoch 101/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2743 - MSE: 0.0881\n",
      "Epoch 102/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2688 - MSE: 0.0878\n",
      "Epoch 103/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2632 - MSE: 0.0874\n",
      "Epoch 104/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2584 - MSE: 0.0869\n",
      "Epoch 105/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2537 - MSE: 0.0865\n",
      "Epoch 106/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2488 - MSE: 0.0860\n",
      "Epoch 107/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2441 - MSE: 0.0859\n",
      "Epoch 108/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2394 - MSE: 0.0852\n",
      "Epoch 109/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.2355 - MSE: 0.0852\n",
      "Epoch 110/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2309 - MSE: 0.0847\n",
      "Epoch 111/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2265 - MSE: 0.0844\n",
      "Epoch 112/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2226 - MSE: 0.0835\n",
      "Epoch 113/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2186 - MSE: 0.0838\n",
      "Epoch 114/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2146 - MSE: 0.0831\n",
      "Epoch 115/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2109 - MSE: 0.0831\n",
      "Epoch 116/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2071 - MSE: 0.0827\n",
      "Epoch 117/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2032 - MSE: 0.0822\n",
      "Epoch 118/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1997 - MSE: 0.0819\n",
      "Epoch 119/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1962 - MSE: 0.0816\n",
      "Epoch 120/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1929 - MSE: 0.0813\n",
      "Epoch 121/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1894 - MSE: 0.0809\n",
      "Epoch 122/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1860 - MSE: 0.0804\n",
      "Epoch 123/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1828 - MSE: 0.0804\n",
      "Epoch 124/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1801 - MSE: 0.0800\n",
      "Epoch 125/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1768 - MSE: 0.0796\n",
      "Epoch 126/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1738 - MSE: 0.0792\n",
      "Epoch 127/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1710 - MSE: 0.0791\n",
      "Epoch 128/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1677 - MSE: 0.0787\n",
      "Epoch 129/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1649 - MSE: 0.0784\n",
      "Epoch 130/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1621 - MSE: 0.0781\n",
      "Epoch 131/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1593 - MSE: 0.0780\n",
      "Epoch 132/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1566 - MSE: 0.0774\n",
      "Epoch 133/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1539 - MSE: 0.0772\n",
      "Epoch 134/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1526 - MSE: 0.0772\n",
      "Epoch 135/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1486 - MSE: 0.0766\n",
      "Epoch 136/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1471 - MSE: 0.0765\n",
      "Epoch 137/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1442 - MSE: 0.0761\n",
      "Epoch 138/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1416 - MSE: 0.0758\n",
      "Epoch 139/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1394 - MSE: 0.0758\n",
      "Epoch 140/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1372 - MSE: 0.0753\n",
      "Epoch 141/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1348 - MSE: 0.0748\n",
      "Epoch 142/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1329 - MSE: 0.0749\n",
      "Epoch 143/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1304 - MSE: 0.0745\n",
      "Epoch 144/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1283 - MSE: 0.0741\n",
      "Epoch 145/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1263 - MSE: 0.0740\n",
      "Epoch 146/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1245 - MSE: 0.0737\n",
      "Epoch 147/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1225 - MSE: 0.0734\n",
      "Epoch 148/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1204 - MSE: 0.0731\n",
      "Epoch 149/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1186 - MSE: 0.0731\n",
      "Epoch 150/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - MSE: 0.0726\n",
      "Epoch 151/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1150 - MSE: 0.0724\n",
      "Epoch 152/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.1130 - MSE: 0.0721\n",
      "Epoch 153/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1112 - MSE: 0.0718\n",
      "Epoch 154/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.1096 - MSE: 0.0717\n",
      "Epoch 155/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1081 - MSE: 0.0714\n",
      "Epoch 156/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1062 - MSE: 0.0711\n",
      "Epoch 157/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1048 - MSE: 0.0711\n",
      "Epoch 158/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.1033 - MSE: 0.0706\n",
      "Epoch 159/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1014 - MSE: 0.0700\n",
      "Epoch 160/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0999 - MSE: 0.0699\n",
      "Epoch 161/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0983 - MSE: 0.0698\n",
      "Epoch 162/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0969 - MSE: 0.0695\n",
      "Epoch 163/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0952 - MSE: 0.0693\n",
      "Epoch 164/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0939 - MSE: 0.0690\n",
      "Epoch 165/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0925 - MSE: 0.0689\n",
      "Epoch 166/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0911 - MSE: 0.0684\n",
      "Epoch 167/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0897 - MSE: 0.0683\n",
      "Epoch 168/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0885 - MSE: 0.0681\n",
      "Epoch 169/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0873 - MSE: 0.0678\n",
      "Epoch 170/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - MSE: 0.0677\n",
      "Epoch 171/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0843 - MSE: 0.0676\n",
      "Epoch 172/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0832 - MSE: 0.0671\n",
      "Epoch 173/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0818 - MSE: 0.0670\n",
      "Epoch 174/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0807 - MSE: 0.0668\n",
      "Epoch 175/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0797 - MSE: 0.0665\n",
      "Epoch 176/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0783 - MSE: 0.0662\n",
      "Epoch 177/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0771 - MSE: 0.0663\n",
      "Epoch 178/1000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0758 - MSE: 0.0657\n",
      "Epoch 179/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0743 - MSE: 0.0657\n",
      "Epoch 180/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0726 - MSE: 0.0655\n",
      "Epoch 181/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0713 - MSE: 0.0651\n",
      "Epoch 182/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0702 - MSE: 0.0651\n",
      "Epoch 183/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0692 - MSE: 0.0648\n",
      "Epoch 184/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0682 - MSE: 0.0646\n",
      "Epoch 185/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 0.0642\n",
      "Epoch 186/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0661 - MSE: 0.0642\n",
      "Epoch 187/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0653 - MSE: 0.0639\n",
      "Epoch 188/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0644 - MSE: 0.0636\n",
      "Epoch 189/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0634 - MSE: 0.0634\n",
      "Epoch 190/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0626 - MSE: 0.0633\n",
      "Epoch 191/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0616 - MSE: 0.0630\n",
      "Epoch 192/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0607 - MSE: 0.0628\n",
      "Epoch 193/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0599 - MSE: 0.0626\n",
      "Epoch 194/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0590 - MSE: 0.0624\n",
      "Epoch 195/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0582 - MSE: 0.0622\n",
      "Epoch 196/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0574 - MSE: 0.0620\n",
      "Epoch 197/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0566 - MSE: 0.0616\n",
      "Epoch 198/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0559 - MSE: 0.0616\n",
      "Epoch 199/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0552 - MSE: 0.0615\n",
      "Epoch 200/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0544 - MSE: 0.0612\n",
      "Epoch 201/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0536 - MSE: 0.0610\n",
      "Epoch 202/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0529 - MSE: 0.0609\n",
      "Epoch 203/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0522 - MSE: 0.0606\n",
      "Epoch 204/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0511 - MSE: 0.0603\n",
      "Epoch 205/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0502 - MSE: 0.0602\n",
      "Epoch 206/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0489 - MSE: 0.0602\n",
      "Epoch 207/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0473 - MSE: 0.0602\n",
      "Epoch 208/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0461 - MSE: 0.0597\n",
      "Epoch 209/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0450 - MSE: 0.0598\n",
      "Epoch 210/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0441 - MSE: 0.0594\n",
      "Epoch 211/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0435 - MSE: 0.0594\n",
      "Epoch 212/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0430 - MSE: 0.0591\n",
      "Epoch 213/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0421 - MSE: 0.0590\n",
      "Epoch 214/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0414 - MSE: 0.0588\n",
      "Epoch 215/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0409 - MSE: 0.0586\n",
      "Epoch 216/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0402 - MSE: 0.0585\n",
      "Epoch 217/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0396 - MSE: 0.0584\n",
      "Epoch 218/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0388 - MSE: 0.0580\n",
      "Epoch 219/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0382 - MSE: 0.0581\n",
      "Epoch 220/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0377 - MSE: 0.0581\n",
      "Epoch 221/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0371 - MSE: 0.0579\n",
      "Epoch 222/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0364 - MSE: 0.0576\n",
      "Epoch 223/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0356 - MSE: 0.0575\n",
      "Epoch 224/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0350 - MSE: 0.0573\n",
      "Epoch 225/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0344 - MSE: 0.0570\n",
      "Epoch 226/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0339 - MSE: 0.0571\n",
      "Epoch 227/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0331 - MSE: 0.0569\n",
      "Epoch 228/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0326 - MSE: 0.0567\n",
      "Epoch 229/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0320 - MSE: 0.0562\n",
      "Epoch 230/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0314 - MSE: 0.0563\n",
      "Epoch 231/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0309 - MSE: 0.0562\n",
      "Epoch 232/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0303 - MSE: 0.0560\n",
      "Epoch 233/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0298 - MSE: 0.0560\n",
      "Epoch 234/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0293 - MSE: 0.0557\n",
      "Epoch 235/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0289 - MSE: 0.0558\n",
      "Epoch 236/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0284 - MSE: 0.0553\n",
      "Epoch 237/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0279 - MSE: 0.0554\n",
      "Epoch 238/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0275 - MSE: 0.0552\n",
      "Epoch 239/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0270 - MSE: 0.0550\n",
      "Epoch 240/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0266 - MSE: 0.0550\n",
      "Epoch 241/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0262 - MSE: 0.0545\n",
      "Epoch 242/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0258 - MSE: 0.0546\n",
      "Epoch 243/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0253 - MSE: 0.0544\n",
      "Epoch 244/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0250 - MSE: 0.0542\n",
      "Epoch 245/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0245 - MSE: 0.0539\n",
      "Epoch 246/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0242 - MSE: 0.0542\n",
      "Epoch 247/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0238 - MSE: 0.0539\n",
      "Epoch 248/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0232 - MSE: 0.0537\n",
      "Epoch 249/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0229 - MSE: 0.0536\n",
      "Epoch 250/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0225 - MSE: 0.0535\n",
      "Epoch 251/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0221 - MSE: 0.0534\n",
      "Epoch 252/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0218 - MSE: 0.0535\n",
      "Epoch 253/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0213 - MSE: 0.0528\n",
      "Epoch 254/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0209 - MSE: 0.0528\n",
      "Epoch 255/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0206 - MSE: 0.0528\n",
      "Epoch 256/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0202 - MSE: 0.0526\n",
      "Epoch 257/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0199 - MSE: 0.0523\n",
      "Epoch 258/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0196 - MSE: 0.0522\n",
      "Epoch 259/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0193 - MSE: 0.0521\n",
      "Epoch 260/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0189 - MSE: 0.0518\n",
      "Epoch 261/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0186 - MSE: 0.0516\n",
      "Epoch 262/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0184 - MSE: 0.0517\n",
      "Epoch 263/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0180 - MSE: 0.0516\n",
      "Epoch 264/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0176 - MSE: 0.0510\n",
      "Epoch 265/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0173 - MSE: 0.0512\n",
      "Epoch 266/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0170 - MSE: 0.0510\n",
      "Epoch 267/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0166 - MSE: 0.0505\n",
      "Epoch 268/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0163 - MSE: 0.0505\n",
      "Epoch 269/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0160 - MSE: 0.0500\n",
      "Epoch 270/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0158 - MSE: 0.0503\n",
      "Epoch 271/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0153 - MSE: 0.0499\n",
      "Epoch 272/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0151 - MSE: 0.0497\n",
      "Epoch 273/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0148 - MSE: 0.0495\n",
      "Epoch 274/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0145 - MSE: 0.0492\n",
      "Epoch 275/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0143 - MSE: 0.0493\n",
      "Epoch 276/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0140 - MSE: 0.0488\n",
      "Epoch 277/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0138 - MSE: 0.0487\n",
      "Epoch 278/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0136 - MSE: 0.0486\n",
      "Epoch 279/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0134 - MSE: 0.0484\n",
      "Epoch 280/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0131 - MSE: 0.0484\n",
      "Epoch 281/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0129 - MSE: 0.0481\n",
      "Epoch 282/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0127 - MSE: 0.0479\n",
      "Epoch 283/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0125 - MSE: 0.0478\n",
      "Epoch 284/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0123 - MSE: 0.0476\n",
      "Epoch 285/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0121 - MSE: 0.0474\n",
      "Epoch 286/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0118 - MSE: 0.0474\n",
      "Epoch 287/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0116 - MSE: 0.0472\n",
      "Epoch 288/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0114 - MSE: 0.0470\n",
      "Epoch 289/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0112 - MSE: 0.0468\n",
      "Epoch 290/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - MSE: 0.0468\n",
      "Epoch 291/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0108 - MSE: 0.0465\n",
      "Epoch 292/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0106 - MSE: 0.0464\n",
      "Epoch 293/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0105 - MSE: 0.0463\n",
      "Epoch 294/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0103 - MSE: 0.0460\n",
      "Epoch 295/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0101 - MSE: 0.0459\n",
      "Epoch 296/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0099 - MSE: 0.0458\n",
      "Epoch 297/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - MSE: 0.0455\n",
      "Epoch 298/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - MSE: 0.0454\n",
      "Epoch 299/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - MSE: 0.0452\n",
      "Epoch 300/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0093 - MSE: 0.0453\n",
      "Epoch 301/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0091 - MSE: 0.0448\n",
      "Epoch 302/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0090 - MSE: 0.0447\n",
      "Epoch 303/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0089 - MSE: 0.0447\n",
      "Epoch 304/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0087 - MSE: 0.0444\n",
      "Epoch 305/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0086 - MSE: 0.0444\n",
      "Epoch 306/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0084 - MSE: 0.0441\n",
      "Epoch 307/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0083 - MSE: 0.0440\n",
      "Epoch 308/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0082 - MSE: 0.0439\n",
      "Epoch 309/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0081 - MSE: 0.0437\n",
      "Epoch 310/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0079 - MSE: 0.0437\n",
      "Epoch 311/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0078 - MSE: 0.0434\n",
      "Epoch 312/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0077 - MSE: 0.0433\n",
      "Epoch 313/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0076 - MSE: 0.0432\n",
      "Epoch 314/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0075 - MSE: 0.0429\n",
      "Epoch 315/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0073 - MSE: 0.0429\n",
      "Epoch 316/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0072 - MSE: 0.0427\n",
      "Epoch 317/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0071 - MSE: 0.0426\n",
      "Epoch 318/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0070 - MSE: 0.0423\n",
      "Epoch 319/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0069 - MSE: 0.0424\n",
      "Epoch 320/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0068 - MSE: 0.0422\n",
      "Epoch 321/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0067 - MSE: 0.0421\n",
      "Epoch 322/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0066 - MSE: 0.0419\n",
      "Epoch 323/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0065 - MSE: 0.0418\n",
      "Epoch 324/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0064 - MSE: 0.0416\n",
      "Epoch 325/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0063 - MSE: 0.0416\n",
      "Epoch 326/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0062 - MSE: 0.0413\n",
      "Epoch 327/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0061 - MSE: 0.0412\n",
      "Epoch 328/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0060 - MSE: 0.0410\n",
      "Epoch 329/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0059 - MSE: 0.0409\n",
      "Epoch 330/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0058 - MSE: 0.0409\n",
      "Epoch 331/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0058 - MSE: 0.0407\n",
      "Epoch 332/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0057 - MSE: 0.0405\n",
      "Epoch 333/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0056 - MSE: 0.0403\n",
      "Epoch 334/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0055 - MSE: 0.0402\n",
      "Epoch 335/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0054 - MSE: 0.0401\n",
      "Epoch 336/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0053 - MSE: 0.0400\n",
      "Epoch 337/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0053 - MSE: 0.0398\n",
      "Epoch 338/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0052 - MSE: 0.0398\n",
      "Epoch 339/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0051 - MSE: 0.0396\n",
      "Epoch 340/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0050 - MSE: 0.0395\n",
      "Epoch 341/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0050 - MSE: 0.0394\n",
      "Epoch 342/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0049 - MSE: 0.0391\n",
      "Epoch 343/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0048 - MSE: 0.0390\n",
      "Epoch 344/1000\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.0047 - MSE: 0.0390\n",
      "Epoch 345/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0046 - MSE: 0.0389\n",
      "Epoch 346/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0046 - MSE: 0.0387\n",
      "Epoch 347/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0045 - MSE: 0.0386\n",
      "Epoch 348/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0044 - MSE: 0.0384\n",
      "Epoch 349/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0044 - MSE: 0.0383\n",
      "Epoch 350/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0043 - MSE: 0.0382\n",
      "Epoch 351/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0042 - MSE: 0.0381\n",
      "Epoch 352/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0042 - MSE: 0.0379\n",
      "Epoch 353/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0041 - MSE: 0.0378\n",
      "Epoch 354/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0041 - MSE: 0.0378\n",
      "Epoch 355/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0040 - MSE: 0.0375\n",
      "Epoch 356/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0039 - MSE: 0.0375\n",
      "Epoch 357/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0039 - MSE: 0.0374\n",
      "Epoch 358/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0038 - MSE: 0.0373\n",
      "Epoch 359/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0037 - MSE: 0.0372\n",
      "Epoch 360/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0037 - MSE: 0.0369\n",
      "Epoch 361/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0036 - MSE: 0.0369\n",
      "Epoch 362/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0036 - MSE: 0.0368\n",
      "Epoch 363/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0035 - MSE: 0.0367\n",
      "Epoch 364/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0035 - MSE: 0.0365\n",
      "Epoch 365/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0034 - MSE: 0.0364\n",
      "Epoch 366/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0034 - MSE: 0.0363\n",
      "Epoch 367/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0033 - MSE: 0.0362\n",
      "Epoch 368/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0033 - MSE: 0.0360\n",
      "Epoch 369/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0032 - MSE: 0.0360\n",
      "Epoch 370/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - MSE: 0.0358\n",
      "Epoch 371/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - MSE: 0.0357\n",
      "Epoch 372/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0031 - MSE: 0.0355\n",
      "Epoch 373/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0030 - MSE: 0.0355\n",
      "Epoch 374/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0030 - MSE: 0.0354\n",
      "Epoch 375/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0029 - MSE: 0.0353\n",
      "Epoch 376/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0029 - MSE: 0.0352\n",
      "Epoch 377/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0028 - MSE: 0.0350\n",
      "Epoch 378/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0028 - MSE: 0.0350\n",
      "Epoch 379/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0027 - MSE: 0.0347\n",
      "Epoch 380/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0027 - MSE: 0.0346\n",
      "Epoch 381/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0027 - MSE: 0.0347\n",
      "Epoch 382/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0026 - MSE: 0.0345\n",
      "Epoch 383/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0026 - MSE: 0.0343\n",
      "Epoch 384/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0025 - MSE: 0.0344\n",
      "Epoch 385/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0025 - MSE: 0.0341\n",
      "Epoch 386/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0025 - MSE: 0.0340\n",
      "Epoch 387/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0024 - MSE: 0.0340\n",
      "Epoch 388/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0024 - MSE: 0.0339\n",
      "Epoch 389/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0024 - MSE: 0.0337\n",
      "Epoch 390/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0023 - MSE: 0.0337\n",
      "Epoch 391/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0023 - MSE: 0.0336\n",
      "Epoch 392/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0023 - MSE: 0.0333\n",
      "Epoch 393/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0022 - MSE: 0.0335\n",
      "Epoch 394/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0022 - MSE: 0.0332\n",
      "Epoch 395/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0022 - MSE: 0.0332\n",
      "Epoch 396/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0021 - MSE: 0.0331\n",
      "Epoch 397/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0021 - MSE: 0.0330\n",
      "Epoch 398/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0021 - MSE: 0.0329\n",
      "Epoch 399/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0020 - MSE: 0.0328\n",
      "Epoch 400/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0020 - MSE: 0.0327\n",
      "Epoch 401/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0020 - MSE: 0.0326\n",
      "Epoch 402/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0019 - MSE: 0.0326\n",
      "Epoch 403/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0019 - MSE: 0.0324\n",
      "Epoch 404/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0019 - MSE: 0.0325\n",
      "Epoch 405/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0019 - MSE: 0.0323\n",
      "Epoch 406/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0018 - MSE: 0.0321\n",
      "Epoch 407/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0018 - MSE: 0.0321\n",
      "Epoch 408/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0018 - MSE: 0.0319\n",
      "Epoch 409/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0018 - MSE: 0.0319\n",
      "Epoch 410/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0017 - MSE: 0.0318\n",
      "Epoch 411/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0017 - MSE: 0.0317\n",
      "Epoch 412/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0017 - MSE: 0.0316\n",
      "Epoch 413/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0017 - MSE: 0.0315\n",
      "Epoch 414/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0016 - MSE: 0.0315\n",
      "Epoch 415/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0016 - MSE: 0.0313\n",
      "Epoch 416/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0016 - MSE: 0.0312\n",
      "Epoch 417/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0016 - MSE: 0.0311\n",
      "Epoch 418/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0015 - MSE: 0.0311\n",
      "Epoch 419/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0015 - MSE: 0.0309\n",
      "Epoch 420/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0015 - MSE: 0.0308\n",
      "Epoch 421/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0015 - MSE: 0.0308\n",
      "Epoch 422/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0014 - MSE: 0.0307\n",
      "Epoch 423/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0014 - MSE: 0.0306\n",
      "Epoch 424/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0014 - MSE: 0.0305\n",
      "Epoch 425/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0014 - MSE: 0.0304\n",
      "Epoch 426/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0014 - MSE: 0.0304\n",
      "Epoch 427/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0013 - MSE: 0.0302\n",
      "Epoch 428/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 0.0013 - MSE: 0.0302\n",
      "Epoch 429/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0013 - MSE: 0.0301\n",
      "Epoch 430/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0013 - MSE: 0.0299\n",
      "Epoch 431/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0013 - MSE: 0.0299\n",
      "Epoch 432/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0013 - MSE: 0.0299\n",
      "Epoch 433/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0296\n",
      "Epoch 434/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0296\n",
      "Epoch 435/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0296\n",
      "Epoch 436/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0296\n",
      "Epoch 437/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0294\n",
      "Epoch 438/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0012 - MSE: 0.0293\n",
      "Epoch 439/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0292\n",
      "Epoch 440/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0292\n",
      "Epoch 441/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0291\n",
      "Epoch 442/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0290\n",
      "Epoch 443/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0289\n",
      "Epoch 444/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0011 - MSE: 0.0289\n",
      "Epoch 445/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0010 - MSE: 0.0288\n",
      "Epoch 446/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0010 - MSE: 0.0287\n",
      "Epoch 447/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0010 - MSE: 0.0286\n",
      "Epoch 448/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.9698e-04 - MSE: 0.0285\n",
      "Epoch 449/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.8072e-04 - MSE: 0.0285\n",
      "Epoch 450/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.6787e-04 - MSE: 0.0284\n",
      "Epoch 451/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.5233e-04 - MSE: 0.0282\n",
      "Epoch 452/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.4054e-04 - MSE: 0.0282\n",
      "Epoch 453/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.2765e-04 - MSE: 0.0282\n",
      "Epoch 454/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.1183e-04 - MSE: 0.0281\n",
      "Epoch 455/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.0064e-04 - MSE: 0.0280\n",
      "Epoch 456/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.8826e-04 - MSE: 0.0280\n",
      "Epoch 457/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.7529e-04 - MSE: 0.0279\n",
      "Epoch 458/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.6133e-04 - MSE: 0.0277\n",
      "Epoch 459/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.4925e-04 - MSE: 0.0277\n",
      "Epoch 460/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.3702e-04 - MSE: 0.0277\n",
      "Epoch 461/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.2503e-04 - MSE: 0.0276\n",
      "Epoch 462/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.1537e-04 - MSE: 0.0274\n",
      "Epoch 463/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.0258e-04 - MSE: 0.0276\n",
      "Epoch 464/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.9005e-04 - MSE: 0.0273\n",
      "Epoch 465/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.7905e-04 - MSE: 0.0273\n",
      "Epoch 466/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.6791e-04 - MSE: 0.0273\n",
      "Epoch 467/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.5595e-04 - MSE: 0.0272\n",
      "Epoch 468/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.4546e-04 - MSE: 0.0271\n",
      "Epoch 469/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.3500e-04 - MSE: 0.0270\n",
      "Epoch 470/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.2551e-04 - MSE: 0.0269\n",
      "Epoch 471/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.1518e-04 - MSE: 0.0269\n",
      "Epoch 472/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.0406e-04 - MSE: 0.0268\n",
      "Epoch 473/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.9440e-04 - MSE: 0.0268\n",
      "Epoch 474/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.8423e-04 - MSE: 0.0267\n",
      "Epoch 475/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.7406e-04 - MSE: 0.0266\n",
      "Epoch 476/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.6448e-04 - MSE: 0.0265\n",
      "Epoch 477/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.5515e-04 - MSE: 0.0265\n",
      "Epoch 478/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.4593e-04 - MSE: 0.0263\n",
      "Epoch 479/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.3602e-04 - MSE: 0.0262\n",
      "Epoch 480/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2764e-04 - MSE: 0.0262\n",
      "Epoch 481/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.1988e-04 - MSE: 0.0261\n",
      "Epoch 482/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0938e-04 - MSE: 0.0261\n",
      "Epoch 483/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0205e-04 - MSE: 0.0260\n",
      "Epoch 484/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.9370e-04 - MSE: 0.0260\n",
      "Epoch 485/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.8553e-04 - MSE: 0.0260\n",
      "Epoch 486/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.7700e-04 - MSE: 0.0258\n",
      "Epoch 487/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.6795e-04 - MSE: 0.0257\n",
      "Epoch 488/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.6008e-04 - MSE: 0.0257\n",
      "Epoch 489/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.5109e-04 - MSE: 0.0256\n",
      "Epoch 490/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.4456e-04 - MSE: 0.0256\n",
      "Epoch 491/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.3703e-04 - MSE: 0.0254\n",
      "Epoch 492/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.2902e-04 - MSE: 0.0254\n",
      "Epoch 493/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.2119e-04 - MSE: 0.0253\n",
      "Epoch 494/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.1402e-04 - MSE: 0.0253\n",
      "Epoch 495/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.0796e-04 - MSE: 0.0252\n",
      "Epoch 496/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.0132e-04 - MSE: 0.0252\n",
      "Epoch 497/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.9389e-04 - MSE: 0.0250\n",
      "Epoch 498/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.8560e-04 - MSE: 0.0250\n",
      "Epoch 499/1000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.7945e-04 - MSE: 0.0249\n",
      "Epoch 500/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.7268e-04 - MSE: 0.0249\n",
      "Epoch 501/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.6612e-04 - MSE: 0.0249\n",
      "Epoch 502/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5873e-04 - MSE: 0.0248\n",
      "Epoch 503/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5281e-04 - MSE: 0.0247\n",
      "Epoch 504/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.4566e-04 - MSE: 0.0247\n",
      "Epoch 505/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.3989e-04 - MSE: 0.0247\n",
      "Epoch 506/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.3303e-04 - MSE: 0.0245\n",
      "Epoch 507/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2702e-04 - MSE: 0.0245\n",
      "Epoch 508/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2162e-04 - MSE: 0.0244\n",
      "Epoch 509/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.1457e-04 - MSE: 0.0243\n",
      "Epoch 510/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.0938e-04 - MSE: 0.0243\n",
      "Epoch 511/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.0356e-04 - MSE: 0.0242\n",
      "Epoch 512/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.9804e-04 - MSE: 0.0242\n",
      "Epoch 513/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.9179e-04 - MSE: 0.0241\n",
      "Epoch 514/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8695e-04 - MSE: 0.0241\n",
      "Epoch 515/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8187e-04 - MSE: 0.0240\n",
      "Epoch 516/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7567e-04 - MSE: 0.0239\n",
      "Epoch 517/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7040e-04 - MSE: 0.0239\n",
      "Epoch 518/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6492e-04 - MSE: 0.0238\n",
      "Epoch 519/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6015e-04 - MSE: 0.0238\n",
      "Epoch 520/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5518e-04 - MSE: 0.0237\n",
      "Epoch 521/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5042e-04 - MSE: 0.0237\n",
      "Epoch 522/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4540e-04 - MSE: 0.0236\n",
      "Epoch 523/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3993e-04 - MSE: 0.0235\n",
      "Epoch 524/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3571e-04 - MSE: 0.0235\n",
      "Epoch 525/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3112e-04 - MSE: 0.0234\n",
      "Epoch 526/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2648e-04 - MSE: 0.0234\n",
      "Epoch 527/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2142e-04 - MSE: 0.0233\n",
      "Epoch 528/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1711e-04 - MSE: 0.0233\n",
      "Epoch 529/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1207e-04 - MSE: 0.0233\n",
      "Epoch 530/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.0803e-04 - MSE: 0.0232\n",
      "Epoch 531/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.0363e-04 - MSE: 0.0231\n",
      "Epoch 532/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9924e-04 - MSE: 0.0231\n",
      "Epoch 533/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9493e-04 - MSE: 0.0230\n",
      "Epoch 534/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9110e-04 - MSE: 0.0230\n",
      "Epoch 535/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8699e-04 - MSE: 0.0229\n",
      "Epoch 536/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8290e-04 - MSE: 0.0228\n",
      "Epoch 537/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7874e-04 - MSE: 0.0229\n",
      "Epoch 538/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7462e-04 - MSE: 0.0228\n",
      "Epoch 539/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7119e-04 - MSE: 0.0227\n",
      "Epoch 540/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6716e-04 - MSE: 0.0227\n",
      "Epoch 541/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6345e-04 - MSE: 0.0226\n",
      "Epoch 542/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5998e-04 - MSE: 0.0226\n",
      "Epoch 543/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5610e-04 - MSE: 0.0225\n",
      "Epoch 544/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5199e-04 - MSE: 0.0225\n",
      "Epoch 545/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4839e-04 - MSE: 0.0224\n",
      "Epoch 546/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4525e-04 - MSE: 0.0224\n",
      "Epoch 547/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4152e-04 - MSE: 0.0223\n",
      "Epoch 548/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3824e-04 - MSE: 0.0223\n",
      "Epoch 549/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3526e-04 - MSE: 0.0222\n",
      "Epoch 550/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3150e-04 - MSE: 0.0221\n",
      "Epoch 551/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2813e-04 - MSE: 0.0222\n",
      "Epoch 552/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2518e-04 - MSE: 0.0221\n",
      "Epoch 553/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2187e-04 - MSE: 0.0221\n",
      "Epoch 554/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1869e-04 - MSE: 0.0220\n",
      "Epoch 555/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1566e-04 - MSE: 0.0219\n",
      "Epoch 556/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1306e-04 - MSE: 0.0220\n",
      "Epoch 557/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0955e-04 - MSE: 0.0218\n",
      "Epoch 558/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0646e-04 - MSE: 0.0218\n",
      "Epoch 559/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0359e-04 - MSE: 0.0218\n",
      "Epoch 560/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0077e-04 - MSE: 0.0217\n",
      "Epoch 561/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9792e-04 - MSE: 0.0217\n",
      "Epoch 562/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9505e-04 - MSE: 0.0217\n",
      "Epoch 563/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9218e-04 - MSE: 0.0216\n",
      "Epoch 564/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8929e-04 - MSE: 0.0216\n",
      "Epoch 565/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8657e-04 - MSE: 0.0215\n",
      "Epoch 566/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8411e-04 - MSE: 0.0215\n",
      "Epoch 567/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8058e-04 - MSE: 0.0213\n",
      "Epoch 568/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7891e-04 - MSE: 0.0213\n",
      "Epoch 569/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7587e-04 - MSE: 0.0212\n",
      "Epoch 570/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7330e-04 - MSE: 0.0213\n",
      "Epoch 571/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7104e-04 - MSE: 0.0211\n",
      "Epoch 572/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6852e-04 - MSE: 0.0212\n",
      "Epoch 573/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6626e-04 - MSE: 0.0212\n",
      "Epoch 574/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6377e-04 - MSE: 0.0211\n",
      "Epoch 575/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6148e-04 - MSE: 0.0210\n",
      "Epoch 576/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5899e-04 - MSE: 0.0211\n",
      "Epoch 577/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5686e-04 - MSE: 0.0210\n",
      "Epoch 578/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5468e-04 - MSE: 0.0210\n",
      "Epoch 579/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5226e-04 - MSE: 0.0210\n",
      "Epoch 580/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5018e-04 - MSE: 0.0209\n",
      "Epoch 581/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4829e-04 - MSE: 0.0209\n",
      "Epoch 582/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4617e-04 - MSE: 0.0209\n",
      "Epoch 583/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4425e-04 - MSE: 0.0209\n",
      "Epoch 584/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4196e-04 - MSE: 0.0208\n",
      "Epoch 585/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3987e-04 - MSE: 0.0208\n",
      "Epoch 586/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3811e-04 - MSE: 0.0207\n",
      "Epoch 587/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3612e-04 - MSE: 0.0207\n",
      "Epoch 588/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3399e-04 - MSE: 0.0207\n",
      "Epoch 589/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3235e-04 - MSE: 0.0207\n",
      "Epoch 590/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3056e-04 - MSE: 0.0206\n",
      "Epoch 591/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2874e-04 - MSE: 0.0205\n",
      "Epoch 592/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2685e-04 - MSE: 0.0205\n",
      "Epoch 593/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2500e-04 - MSE: 0.0205\n",
      "Epoch 594/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2313e-04 - MSE: 0.0205\n",
      "Epoch 595/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2139e-04 - MSE: 0.0204\n",
      "Epoch 596/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1995e-04 - MSE: 0.0204\n",
      "Epoch 597/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1806e-04 - MSE: 0.0204\n",
      "Epoch 598/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1638e-04 - MSE: 0.0204\n",
      "Epoch 599/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1461e-04 - MSE: 0.0204\n",
      "Epoch 600/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1340e-04 - MSE: 0.0203\n",
      "Epoch 601/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1179e-04 - MSE: 0.0203\n",
      "Epoch 602/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0984e-04 - MSE: 0.0203\n",
      "Epoch 603/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0835e-04 - MSE: 0.0202\n",
      "Epoch 604/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0683e-04 - MSE: 0.0202\n",
      "Epoch 605/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0506e-04 - MSE: 0.0202\n",
      "Epoch 606/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0363e-04 - MSE: 0.0201\n",
      "Epoch 607/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0231e-04 - MSE: 0.0201\n",
      "Epoch 608/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0086e-04 - MSE: 0.0200\n",
      "Epoch 609/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.9595e-05 - MSE: 0.0200\n",
      "Epoch 610/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.7809e-05 - MSE: 0.0200\n",
      "Epoch 611/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.6389e-05 - MSE: 0.0200\n",
      "Epoch 612/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.4956e-05 - MSE: 0.0199\n",
      "Epoch 613/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.3745e-05 - MSE: 0.0199\n",
      "Epoch 614/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.2552e-05 - MSE: 0.0199\n",
      "Epoch 615/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.1136e-05 - MSE: 0.0198\n",
      "Epoch 616/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.9825e-05 - MSE: 0.0199\n",
      "Epoch 617/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.8523e-05 - MSE: 0.0198\n",
      "Epoch 618/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.7306e-05 - MSE: 0.0198\n",
      "Epoch 619/1000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.6079e-05 - MSE: 0.0197\n",
      "Epoch 620/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.4875e-05 - MSE: 0.0197\n",
      "Epoch 621/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.3550e-05 - MSE: 0.0197\n",
      "Epoch 622/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.2336e-05 - MSE: 0.0197\n",
      "Epoch 623/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.1150e-05 - MSE: 0.0197\n",
      "Epoch 624/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.0162e-05 - MSE: 0.0196\n",
      "Epoch 625/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.8891e-05 - MSE: 0.0196\n",
      "Epoch 626/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.7764e-05 - MSE: 0.0196\n",
      "Epoch 627/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.6619e-05 - MSE: 0.0195\n",
      "Epoch 628/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.5578e-05 - MSE: 0.0196\n",
      "Epoch 629/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.4576e-05 - MSE: 0.0195\n",
      "Epoch 630/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.3480e-05 - MSE: 0.0195\n",
      "Epoch 631/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.2449e-05 - MSE: 0.0195\n",
      "Epoch 632/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.1419e-05 - MSE: 0.0195\n",
      "Epoch 633/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.0329e-05 - MSE: 0.0195\n",
      "Epoch 634/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.9384e-05 - MSE: 0.0194\n",
      "Epoch 635/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.8381e-05 - MSE: 0.0194\n",
      "Epoch 636/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.7317e-05 - MSE: 0.0194\n",
      "Epoch 637/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.6409e-05 - MSE: 0.0193\n",
      "Epoch 638/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.5593e-05 - MSE: 0.0193\n",
      "Epoch 639/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.4632e-05 - MSE: 0.0194\n",
      "Epoch 640/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.3829e-05 - MSE: 0.0193\n",
      "Epoch 641/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2860e-05 - MSE: 0.0193\n",
      "Epoch 642/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.1973e-05 - MSE: 0.0193\n",
      "Epoch 643/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.1039e-05 - MSE: 0.0192\n",
      "Epoch 644/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0155e-05 - MSE: 0.0192\n",
      "Epoch 645/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.9222e-05 - MSE: 0.0192\n",
      "Epoch 646/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.8492e-05 - MSE: 0.0192\n",
      "Epoch 647/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.7623e-05 - MSE: 0.0192\n",
      "Epoch 648/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0547e-05 - MSE: 0.0192\n",
      "Epoch 649/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.8455e-05 - MSE: 0.0200\n",
      "Epoch 650/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.6411e-05 - MSE: 0.0195\n",
      "Epoch 651/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0130e-05 - MSE: 0.0197\n",
      "Epoch 652/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.5955e-05 - MSE: 0.0195\n",
      "Epoch 653/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.4758e-05 - MSE: 0.0194\n",
      "Epoch 654/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.3860e-05 - MSE: 0.0194\n",
      "Epoch 655/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.3038e-05 - MSE: 0.0192\n",
      "Epoch 656/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.1395e-05 - MSE: 0.0193\n",
      "Epoch 657/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.0417e-05 - MSE: 0.0192\n",
      "Epoch 658/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.9660e-05 - MSE: 0.0191\n",
      "Epoch 659/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.8973e-05 - MSE: 0.0191\n",
      "Epoch 660/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.8085e-05 - MSE: 0.0191\n",
      "Epoch 661/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.7375e-05 - MSE: 0.0190\n",
      "Epoch 662/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.6730e-05 - MSE: 0.0191\n",
      "Epoch 663/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5920e-05 - MSE: 0.0190\n",
      "Epoch 664/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5294e-05 - MSE: 0.0190\n",
      "Epoch 665/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 4.4679e-05 - MSE: 0.0190\n",
      "Epoch 666/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.4055e-05 - MSE: 0.0189\n",
      "Epoch 667/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.3338e-05 - MSE: 0.0189\n",
      "Epoch 668/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2768e-05 - MSE: 0.0189\n",
      "Epoch 669/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2170e-05 - MSE: 0.0189\n",
      "Epoch 670/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.1569e-05 - MSE: 0.0189\n",
      "Epoch 671/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.1021e-05 - MSE: 0.0189\n",
      "Epoch 672/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.0469e-05 - MSE: 0.0188\n",
      "Epoch 673/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.9856e-05 - MSE: 0.0188\n",
      "Epoch 674/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.9368e-05 - MSE: 0.0188\n",
      "Epoch 675/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8793e-05 - MSE: 0.0187\n",
      "Epoch 676/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8216e-05 - MSE: 0.0188\n",
      "Epoch 677/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7658e-05 - MSE: 0.0187\n",
      "Epoch 678/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7207e-05 - MSE: 0.0187\n",
      "Epoch 679/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6640e-05 - MSE: 0.0187\n",
      "Epoch 680/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6155e-05 - MSE: 0.0187\n",
      "Epoch 681/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5641e-05 - MSE: 0.0187\n",
      "Epoch 682/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5217e-05 - MSE: 0.0187\n",
      "Epoch 683/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4722e-05 - MSE: 0.0187\n",
      "Epoch 684/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4303e-05 - MSE: 0.0186\n",
      "Epoch 685/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3793e-05 - MSE: 0.0187\n",
      "Epoch 686/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3349e-05 - MSE: 0.0187\n",
      "Epoch 687/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2891e-05 - MSE: 0.0187\n",
      "Epoch 688/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2438e-05 - MSE: 0.0186\n",
      "Epoch 689/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2002e-05 - MSE: 0.0186\n",
      "Epoch 690/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1528e-05 - MSE: 0.0186\n",
      "Epoch 691/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1120e-05 - MSE: 0.0186\n",
      "Epoch 692/1000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3.0695e-05 - MSE: 0.0186\n",
      "Epoch 693/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.0299e-05 - MSE: 0.0186\n",
      "Epoch 694/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9910e-05 - MSE: 0.0185\n",
      "Epoch 695/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9465e-05 - MSE: 0.0185\n",
      "Epoch 696/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9087e-05 - MSE: 0.0185\n",
      "Epoch 697/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8649e-05 - MSE: 0.0185\n",
      "Epoch 698/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8285e-05 - MSE: 0.0185\n",
      "Epoch 699/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7896e-05 - MSE: 0.0185\n",
      "Epoch 700/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7517e-05 - MSE: 0.0185\n",
      "Epoch 701/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7107e-05 - MSE: 0.0185\n",
      "Epoch 702/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6765e-05 - MSE: 0.0185\n",
      "Epoch 703/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6386e-05 - MSE: 0.0185\n",
      "Epoch 704/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6128e-05 - MSE: 0.0185\n",
      "Epoch 705/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5663e-05 - MSE: 0.0185\n",
      "Epoch 706/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5330e-05 - MSE: 0.0184\n",
      "Epoch 707/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5004e-05 - MSE: 0.0185\n",
      "Epoch 708/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4642e-05 - MSE: 0.0184\n",
      "Epoch 709/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4314e-05 - MSE: 0.0184\n",
      "Epoch 710/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3988e-05 - MSE: 0.0184\n",
      "Epoch 711/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3699e-05 - MSE: 0.0184\n",
      "Epoch 712/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3311e-05 - MSE: 0.0184\n",
      "Epoch 713/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2989e-05 - MSE: 0.0183\n",
      "Epoch 714/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2675e-05 - MSE: 0.0183\n",
      "Epoch 715/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2408e-05 - MSE: 0.0183\n",
      "Epoch 716/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2078e-05 - MSE: 0.0184\n",
      "Epoch 717/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1764e-05 - MSE: 0.0183\n",
      "Epoch 718/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1460e-05 - MSE: 0.0183\n",
      "Epoch 719/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1164e-05 - MSE: 0.0183\n",
      "Epoch 720/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0882e-05 - MSE: 0.0183\n",
      "Epoch 721/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0637e-05 - MSE: 0.0182\n",
      "Epoch 722/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0338e-05 - MSE: 0.0183\n",
      "Epoch 723/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0012e-05 - MSE: 0.0183\n",
      "Epoch 724/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9746e-05 - MSE: 0.0183\n",
      "Epoch 725/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9487e-05 - MSE: 0.0183\n",
      "Epoch 726/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9211e-05 - MSE: 0.0182\n",
      "Epoch 727/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8937e-05 - MSE: 0.0182\n",
      "Epoch 728/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8681e-05 - MSE: 0.0182\n",
      "Epoch 729/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8411e-05 - MSE: 0.0182\n",
      "Epoch 730/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8171e-05 - MSE: 0.0182\n",
      "Epoch 731/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7899e-05 - MSE: 0.0182\n",
      "Epoch 732/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7682e-05 - MSE: 0.0182\n",
      "Epoch 733/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7432e-05 - MSE: 0.0182\n",
      "Epoch 734/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7201e-05 - MSE: 0.0182\n",
      "Epoch 735/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6956e-05 - MSE: 0.0182\n",
      "Epoch 736/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6752e-05 - MSE: 0.0181\n",
      "Epoch 737/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6536e-05 - MSE: 0.0181\n",
      "Epoch 738/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6444e-05 - MSE: 0.0181\n",
      "Epoch 739/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6474e-05 - MSE: 0.0182\n",
      "Epoch 740/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6378e-05 - MSE: 0.0181\n",
      "Epoch 741/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6517e-05 - MSE: 0.0182\n",
      "Epoch 742/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5511e-05 - MSE: 0.0181\n",
      "Epoch 743/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5232e-05 - MSE: 0.0181\n",
      "Epoch 744/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5002e-05 - MSE: 0.0181\n",
      "Epoch 745/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4792e-05 - MSE: 0.0181\n",
      "Epoch 746/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4565e-05 - MSE: 0.0181\n",
      "Epoch 747/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4393e-05 - MSE: 0.0181\n",
      "Epoch 748/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4195e-05 - MSE: 0.0181\n",
      "Epoch 749/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3983e-05 - MSE: 0.0181\n",
      "Epoch 750/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3793e-05 - MSE: 0.0181\n",
      "Epoch 751/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3632e-05 - MSE: 0.0181\n",
      "Epoch 752/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3445e-05 - MSE: 0.0181\n",
      "Epoch 753/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3246e-05 - MSE: 0.0181\n",
      "Epoch 754/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3059e-05 - MSE: 0.0181\n",
      "Epoch 755/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2888e-05 - MSE: 0.0180\n",
      "Epoch 756/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2725e-05 - MSE: 0.0181\n",
      "Epoch 757/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2539e-05 - MSE: 0.0181\n",
      "Epoch 758/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2367e-05 - MSE: 0.0180\n",
      "Epoch 759/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2212e-05 - MSE: 0.0181\n",
      "Epoch 760/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2040e-05 - MSE: 0.0180\n",
      "Epoch 761/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1879e-05 - MSE: 0.0180\n",
      "Epoch 762/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1730e-05 - MSE: 0.0180\n",
      "Epoch 763/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1571e-05 - MSE: 0.0180\n",
      "Epoch 764/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1420e-05 - MSE: 0.0180\n",
      "Epoch 765/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1262e-05 - MSE: 0.0180\n",
      "Epoch 766/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1108e-05 - MSE: 0.0180\n",
      "Epoch 767/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0955e-05 - MSE: 0.0180\n",
      "Epoch 768/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0816e-05 - MSE: 0.0180\n",
      "Epoch 769/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0672e-05 - MSE: 0.0180\n",
      "Epoch 770/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0524e-05 - MSE: 0.0180\n",
      "Epoch 771/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0387e-05 - MSE: 0.0180\n",
      "Epoch 772/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0253e-05 - MSE: 0.0180\n",
      "Epoch 773/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0109e-05 - MSE: 0.0180\n",
      "Epoch 774/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.9791e-06 - MSE: 0.0179\n",
      "Epoch 775/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.8507e-06 - MSE: 0.0179\n",
      "Epoch 776/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.7043e-06 - MSE: 0.0180\n",
      "Epoch 777/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.5593e-06 - MSE: 0.0179\n",
      "Epoch 778/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.4375e-06 - MSE: 0.0179\n",
      "Epoch 779/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.3150e-06 - MSE: 0.0180\n",
      "Epoch 780/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.1929e-06 - MSE: 0.0179\n",
      "Epoch 781/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.0794e-06 - MSE: 0.0179\n",
      "Epoch 782/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.9461e-06 - MSE: 0.0179\n",
      "Epoch 783/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.8235e-06 - MSE: 0.0180\n",
      "Epoch 784/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.7003e-06 - MSE: 0.0179\n",
      "Epoch 785/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.5994e-06 - MSE: 0.0179\n",
      "Epoch 786/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.4812e-06 - MSE: 0.0179\n",
      "Epoch 787/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.3665e-06 - MSE: 0.0179\n",
      "Epoch 788/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.2465e-06 - MSE: 0.0179\n",
      "Epoch 789/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.1632e-06 - MSE: 0.0179\n",
      "Epoch 790/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.0503e-06 - MSE: 0.0179\n",
      "Epoch 791/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.9924e-06 - MSE: 0.0179\n",
      "Epoch 792/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.8403e-06 - MSE: 0.0179\n",
      "Epoch 793/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.7305e-06 - MSE: 0.0179\n",
      "Epoch 794/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.5991e-06 - MSE: 0.0178\n",
      "Epoch 795/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.5117e-06 - MSE: 0.0178\n",
      "Epoch 796/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.3851e-06 - MSE: 0.0177\n",
      "Epoch 797/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.3186e-06 - MSE: 0.0178\n",
      "Epoch 798/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.2018e-06 - MSE: 0.0177\n",
      "Epoch 799/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.0996e-06 - MSE: 0.0178\n",
      "Epoch 800/1000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.0108e-06 - MSE: 0.0178\n",
      "Epoch 801/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.9159e-06 - MSE: 0.0178\n",
      "Epoch 802/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.8246e-06 - MSE: 0.0178\n",
      "Epoch 803/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.7274e-06 - MSE: 0.0178\n",
      "Epoch 804/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.6432e-06 - MSE: 0.0179\n",
      "Epoch 805/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.5533e-06 - MSE: 0.0178\n",
      "Epoch 806/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.4637e-06 - MSE: 0.0179\n",
      "Epoch 807/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.3762e-06 - MSE: 0.0179\n",
      "Epoch 808/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2956e-06 - MSE: 0.0178\n",
      "Epoch 809/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2225e-06 - MSE: 0.0178\n",
      "Epoch 810/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.1386e-06 - MSE: 0.0179\n",
      "Epoch 811/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.0637e-06 - MSE: 0.0179\n",
      "Epoch 812/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.9792e-06 - MSE: 0.0179\n",
      "Epoch 813/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.8954e-06 - MSE: 0.0179\n",
      "Epoch 814/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.8134e-06 - MSE: 0.0179\n",
      "Epoch 815/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.7415e-06 - MSE: 0.0179\n",
      "Epoch 816/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.6703e-06 - MSE: 0.0179\n",
      "Epoch 817/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.5896e-06 - MSE: 0.0179\n",
      "Epoch 818/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.5253e-06 - MSE: 0.0179\n",
      "Epoch 819/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.4443e-06 - MSE: 0.0179\n",
      "Epoch 820/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.3694e-06 - MSE: 0.0179\n",
      "Epoch 821/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.3017e-06 - MSE: 0.0179\n",
      "Epoch 822/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.2234e-06 - MSE: 0.0179\n",
      "Epoch 823/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.1690e-06 - MSE: 0.0179\n",
      "Epoch 824/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.0934e-06 - MSE: 0.0179\n",
      "Epoch 825/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5.0397e-06 - MSE: 0.0179\n",
      "Epoch 826/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.9684e-06 - MSE: 0.0179\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 4.9061e-06 - MSE: 0.0178\n",
      "Epoch 828/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.8349e-06 - MSE: 0.0179\n",
      "Epoch 829/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.7830e-06 - MSE: 0.0179\n",
      "Epoch 830/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.7188e-06 - MSE: 0.0179\n",
      "Epoch 831/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.6557e-06 - MSE: 0.0178\n",
      "Epoch 832/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5793e-06 - MSE: 0.0178\n",
      "Epoch 833/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.5173e-06 - MSE: 0.0178\n",
      "Epoch 834/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.4607e-06 - MSE: 0.0178\n",
      "Epoch 835/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.4015e-06 - MSE: 0.0178\n",
      "Epoch 836/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.3501e-06 - MSE: 0.0178\n",
      "Epoch 837/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2964e-06 - MSE: 0.0178\n",
      "Epoch 838/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.2355e-06 - MSE: 0.0178\n",
      "Epoch 839/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.1993e-06 - MSE: 0.0178\n",
      "Epoch 840/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.1228e-06 - MSE: 0.0178\n",
      "Epoch 841/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.0982e-06 - MSE: 0.0178\n",
      "Epoch 842/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.0231e-06 - MSE: 0.0177\n",
      "Epoch 843/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.9778e-06 - MSE: 0.0178\n",
      "Epoch 844/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8985e-06 - MSE: 0.0178\n",
      "Epoch 845/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.8468e-06 - MSE: 0.0177\n",
      "Epoch 846/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7930e-06 - MSE: 0.0178\n",
      "Epoch 847/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.7464e-06 - MSE: 0.0178\n",
      "Epoch 848/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6948e-06 - MSE: 0.0177\n",
      "Epoch 849/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.6455e-06 - MSE: 0.0177\n",
      "Epoch 850/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5985e-06 - MSE: 0.0177\n",
      "Epoch 851/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5610e-06 - MSE: 0.0177\n",
      "Epoch 852/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5093e-06 - MSE: 0.0177\n",
      "Epoch 853/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4711e-06 - MSE: 0.0177\n",
      "Epoch 854/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4317e-06 - MSE: 0.0177\n",
      "Epoch 855/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.4044e-06 - MSE: 0.0177\n",
      "Epoch 856/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3446e-06 - MSE: 0.0177\n",
      "Epoch 857/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3387e-06 - MSE: 0.0177\n",
      "Epoch 858/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2588e-06 - MSE: 0.0177\n",
      "Epoch 859/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.2170e-06 - MSE: 0.0177\n",
      "Epoch 860/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 3.1567e-06 - MSE: 0.0177\n",
      "Epoch 861/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1195e-06 - MSE: 0.0177\n",
      "Epoch 862/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.0779e-06 - MSE: 0.0177\n",
      "Epoch 863/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.0312e-06 - MSE: 0.0177\n",
      "Epoch 864/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9984e-06 - MSE: 0.0177\n",
      "Epoch 865/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9623e-06 - MSE: 0.0177\n",
      "Epoch 866/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.9212e-06 - MSE: 0.0177\n",
      "Epoch 867/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8798e-06 - MSE: 0.0177\n",
      "Epoch 868/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8446e-06 - MSE: 0.0177\n",
      "Epoch 869/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.8084e-06 - MSE: 0.0178\n",
      "Epoch 870/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7694e-06 - MSE: 0.0177\n",
      "Epoch 871/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.7381e-06 - MSE: 0.0177\n",
      "Epoch 872/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6975e-06 - MSE: 0.0177\n",
      "Epoch 873/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 2.6668e-06 - MSE: 0.0177\n",
      "Epoch 874/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.6360e-06 - MSE: 0.0176\n",
      "Epoch 875/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5993e-06 - MSE: 0.0177\n",
      "Epoch 876/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5665e-06 - MSE: 0.0176\n",
      "Epoch 877/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5392e-06 - MSE: 0.0176\n",
      "Epoch 878/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.5051e-06 - MSE: 0.0177\n",
      "Epoch 879/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4768e-06 - MSE: 0.0177\n",
      "Epoch 880/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4404e-06 - MSE: 0.0177\n",
      "Epoch 881/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.4148e-06 - MSE: 0.0177\n",
      "Epoch 882/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3877e-06 - MSE: 0.0177\n",
      "Epoch 883/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3485e-06 - MSE: 0.0176\n",
      "Epoch 884/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3174e-06 - MSE: 0.0176\n",
      "Epoch 885/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2865e-06 - MSE: 0.0176\n",
      "Epoch 886/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2608e-06 - MSE: 0.0176\n",
      "Epoch 887/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2342e-06 - MSE: 0.0176\n",
      "Epoch 888/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.2069e-06 - MSE: 0.0176\n",
      "Epoch 889/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1840e-06 - MSE: 0.0176\n",
      "Epoch 890/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1591e-06 - MSE: 0.0176\n",
      "Epoch 891/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1373e-06 - MSE: 0.0177\n",
      "Epoch 892/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1077e-06 - MSE: 0.0177\n",
      "Epoch 893/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0792e-06 - MSE: 0.0177\n",
      "Epoch 894/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0516e-06 - MSE: 0.0177\n",
      "Epoch 895/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0246e-06 - MSE: 0.0177\n",
      "Epoch 896/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0020e-06 - MSE: 0.0177\n",
      "Epoch 897/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9743e-06 - MSE: 0.0177\n",
      "Epoch 898/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9541e-06 - MSE: 0.0176\n",
      "Epoch 899/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9283e-06 - MSE: 0.0177\n",
      "Epoch 900/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.9058e-06 - MSE: 0.0177\n",
      "Epoch 901/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8858e-06 - MSE: 0.0177\n",
      "Epoch 902/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8618e-06 - MSE: 0.0177\n",
      "Epoch 903/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8391e-06 - MSE: 0.0177\n",
      "Epoch 904/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8154e-06 - MSE: 0.0177\n",
      "Epoch 905/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7910e-06 - MSE: 0.0177\n",
      "Epoch 906/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7702e-06 - MSE: 0.0177\n",
      "Epoch 907/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7531e-06 - MSE: 0.0177\n",
      "Epoch 908/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7301e-06 - MSE: 0.0177\n",
      "Epoch 909/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.7062e-06 - MSE: 0.0178\n",
      "Epoch 910/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6879e-06 - MSE: 0.0178\n",
      "Epoch 911/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6661e-06 - MSE: 0.0178\n",
      "Epoch 912/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6469e-06 - MSE: 0.0178\n",
      "Epoch 913/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6253e-06 - MSE: 0.0178\n",
      "Epoch 914/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.6060e-06 - MSE: 0.0178\n",
      "Epoch 915/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5863e-06 - MSE: 0.0178\n",
      "Epoch 916/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5684e-06 - MSE: 0.0178\n",
      "Epoch 917/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5497e-06 - MSE: 0.0177\n",
      "Epoch 918/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5259e-06 - MSE: 0.0177\n",
      "Epoch 919/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5114e-06 - MSE: 0.0177\n",
      "Epoch 920/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4944e-06 - MSE: 0.0177\n",
      "Epoch 921/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4760e-06 - MSE: 0.0177\n",
      "Epoch 922/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4597e-06 - MSE: 0.0177\n",
      "Epoch 923/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4455e-06 - MSE: 0.0177\n",
      "Epoch 924/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4292e-06 - MSE: 0.0177\n",
      "Epoch 925/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.4157e-06 - MSE: 0.0177\n",
      "Epoch 926/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3969e-06 - MSE: 0.0177\n",
      "Epoch 927/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3847e-06 - MSE: 0.0177\n",
      "Epoch 928/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3634e-06 - MSE: 0.0177\n",
      "Epoch 929/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3498e-06 - MSE: 0.0177\n",
      "Epoch 930/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3310e-06 - MSE: 0.0177\n",
      "Epoch 931/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3164e-06 - MSE: 0.0178\n",
      "Epoch 932/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2998e-06 - MSE: 0.0177\n",
      "Epoch 933/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2844e-06 - MSE: 0.0178\n",
      "Epoch 934/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2684e-06 - MSE: 0.0178\n",
      "Epoch 935/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2537e-06 - MSE: 0.0177\n",
      "Epoch 936/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2381e-06 - MSE: 0.0177\n",
      "Epoch 937/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2282e-06 - MSE: 0.0178\n",
      "Epoch 938/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2126e-06 - MSE: 0.0178\n",
      "Epoch 939/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2020e-06 - MSE: 0.0178\n",
      "Epoch 940/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1835e-06 - MSE: 0.0178\n",
      "Epoch 941/1000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1704e-06 - MSE: 0.0178\n",
      "Epoch 942/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1570e-06 - MSE: 0.0178\n",
      "Epoch 943/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1440e-06 - MSE: 0.0178\n",
      "Epoch 944/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1309e-06 - MSE: 0.0177\n",
      "Epoch 945/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1183e-06 - MSE: 0.0178\n",
      "Epoch 946/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1077e-06 - MSE: 0.0178\n",
      "Epoch 947/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0911e-06 - MSE: 0.0178\n",
      "Epoch 948/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0818e-06 - MSE: 0.0178\n",
      "Epoch 949/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 1.0676e-06 - MSE: 0.0178\n",
      "Epoch 950/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0586e-06 - MSE: 0.0178\n",
      "Epoch 951/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0470e-06 - MSE: 0.0178\n",
      "Epoch 952/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 1.0366e-06 - MSE: 0.0178\n",
      "Epoch 953/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0247e-06 - MSE: 0.0178\n",
      "Epoch 954/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0143e-06 - MSE: 0.0178\n",
      "Epoch 955/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0046e-06 - MSE: 0.0178\n",
      "Epoch 956/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 9.9694e-07 - MSE: 0.0178\n",
      "Epoch 957/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.8514e-07 - MSE: 0.0178\n",
      "Epoch 958/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 9.7501e-07 - MSE: 0.0178\n",
      "Epoch 959/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 9.6392e-07 - MSE: 0.0178\n",
      "Epoch 960/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 9.5391e-07 - MSE: 0.0178\n",
      "Epoch 961/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 9.4604e-07 - MSE: 0.0178\n",
      "Epoch 962/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 9.3209e-07 - MSE: 0.0178\n",
      "Epoch 963/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.2434e-07 - MSE: 0.0178\n",
      "Epoch 964/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.1350e-07 - MSE: 0.0179\n",
      "Epoch 965/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.0503e-07 - MSE: 0.0178\n",
      "Epoch 966/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.9609e-07 - MSE: 0.0179\n",
      "Epoch 967/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.8632e-07 - MSE: 0.0178\n",
      "Epoch 968/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.7797e-07 - MSE: 0.0179\n",
      "Epoch 969/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.6987e-07 - MSE: 0.0179\n",
      "Epoch 970/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.5949e-07 - MSE: 0.0179\n",
      "Epoch 971/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.5127e-07 - MSE: 0.0179\n",
      "Epoch 972/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.4102e-07 - MSE: 0.0179\n",
      "Epoch 973/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.3088e-07 - MSE: 0.0179\n",
      "Epoch 974/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.2194e-07 - MSE: 0.0179\n",
      "Epoch 975/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 8.1265e-07 - MSE: 0.0179\n",
      "Epoch 976/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 8.0311e-07 - MSE: 0.0179\n",
      "Epoch 977/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.9512e-07 - MSE: 0.0179\n",
      "Epoch 978/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.8714e-07 - MSE: 0.0180\n",
      "Epoch 979/1000\n",
      "32/32 [==============================] - 0s 936us/step - loss: 7.7891e-07 - MSE: 0.0180\n",
      "Epoch 980/1000\n",
      "32/32 [==============================] - 0s 935us/step - loss: 7.6985e-07 - MSE: 0.0179\n",
      "Epoch 981/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.6270e-07 - MSE: 0.0179\n",
      "Epoch 982/1000\n",
      "32/32 [==============================] - 0s 935us/step - loss: 7.5531e-07 - MSE: 0.0180\n",
      "Epoch 983/1000\n",
      "32/32 [==============================] - 0s 935us/step - loss: 7.4696e-07 - MSE: 0.0180\n",
      "Epoch 984/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.4017e-07 - MSE: 0.0180\n",
      "Epoch 985/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.3218e-07 - MSE: 0.0180\n",
      "Epoch 986/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.2527e-07 - MSE: 0.0180\n",
      "Epoch 987/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 7.1728e-07 - MSE: 0.0180\n",
      "Epoch 988/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 7.0870e-07 - MSE: 0.0180\n",
      "Epoch 989/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 7.0202e-07 - MSE: 0.0180\n",
      "Epoch 990/1000\n",
      "32/32 [==============================] - 0s 1000us/step - loss: 6.9165e-07 - MSE: 0.0180\n",
      "Epoch 991/1000\n",
      "32/32 [==============================] - 0s 968us/step - loss: 6.8438e-07 - MSE: 0.0180\n",
      "Epoch 992/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.7579e-07 - MSE: 0.0180\n",
      "Epoch 993/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.7055e-07 - MSE: 0.0180\n",
      "Epoch 994/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.6364e-07 - MSE: 0.0180\n",
      "Epoch 995/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.5565e-07 - MSE: 0.0180\n",
      "Epoch 996/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.5088e-07 - MSE: 0.0181\n",
      "Epoch 997/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.4361e-07 - MSE: 0.0181\n",
      "Epoch 998/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.3622e-07 - MSE: 0.0181\n",
      "Epoch 999/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2954e-07 - MSE: 0.0181\n",
      "Epoch 1000/1000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.2215e-07 - MSE: 0.0181\n"
     ]
    }
   ],
   "source": [
    "history = My_model.fit(X_train_slice, y_train_slice, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66152e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_3/kernel:0' shape=(784, 10) dtype=float32, numpy=\n",
      "array([[-0.08224765,  0.00866737,  0.01973679, ..., -0.0833436 ,\n",
      "        -0.04416414, -0.03904056],\n",
      "       [-0.00581873, -0.0192287 ,  0.05561594, ..., -0.00527091,\n",
      "         0.02650562,  0.05354423],\n",
      "       [ 0.00162055, -0.07965843,  0.06618526, ...,  0.0456444 ,\n",
      "         0.07736488, -0.01622951],\n",
      "       ...,\n",
      "       [-0.06238376, -0.07369419,  0.03138441, ..., -0.08495768,\n",
      "        -0.02173991,  0.06320558],\n",
      "       [-0.01060575, -0.080711  , -0.0062096 , ...,  0.07749363,\n",
      "        -0.0629416 ,  0.00874766],\n",
      "       [ 0.0098411 ,  0.08322633,  0.0851151 , ..., -0.07211091,\n",
      "         0.00502658,  0.0775931 ]], dtype=float32)>, <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.9976949 ,  0.40488276,  0.43169323, -0.10686701,  0.37485948,\n",
      "       -0.71212274, -0.73704106,  1.1697233 ,  0.46051836,  0.8059544 ],\n",
      "      dtype=float32)>]\n",
      "[-0.9976949   0.40488276  0.43169323 -0.10686701  0.37485948 -0.71212274\n",
      " -0.73704106  1.1697233   0.46051836  0.8059544 ]\n"
     ]
    }
   ],
   "source": [
    "print(My_model.layers[1].weights)\n",
    "print(My_model.layers[1].bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "810b14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,bias1 =  My_model.layers[1].weights, My_model.layers[1].bias.numpy() \n",
    "w2,bias2 =  My_model.layers[2].weights, My_model.layers[2].bias.numpy() \n",
    "w3,bias3 =  My_model.layers[3].weights, My_model.layers[3].bias.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "352928e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26944\\334369188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w1.shape: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w2.shape: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w3.shape: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bias1.shape: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bias2.shape: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"w1.shape: \"+str(w1.shape))\n",
    "print(\"w2.shape: \"+str(w2.shape))\n",
    "print(\"w3.shape: \"+str(w3.shape)+\"\\n\")\n",
    "print(\"bias1.shape: \"+str(bias1.shape))\n",
    "print(\"bias2.shape: \"+str(bias2.shape))\n",
    "print(\"bias3.shape: \"+str(bias3.shape)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b877d583",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26944\\4257777964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "z=np.array(w1,dtype = float)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1901ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c766802",
   "metadata": {},
   "source": [
    "### There could be several issues with this code that could cause it to return nan (Not a Number) as the output. Here are some possible issues:\n",
    "\n",
    "1. The initialization of the weights and biases: The weights and biases are initialized using np.random.uniform(-1,1,(784,10)) and np.random.uniform(-1,1,(1,10)), respectively. If the initialization is not done properly, the model may not converge and produce nan as the output.\n",
    "\n",
    "\n",
    "2. The activation function: The activation function used in this code is a sigmoid function. If the input to the sigmoid function is too large or too small, it can result in numerical overflow and produce nan as the output.\n",
    "\n",
    "\n",
    "3. The learning rate: The learning rate is set to a fixed value of 0.0001. If the learning rate is too large, the model may diverge and produce nan as the output. On the other hand, if the learning rate is too small, the model may not converge quickly enough and may require a large number of epochs to train.\n",
    "\n",
    "\n",
    "4. The choice of the loss function: The loss function used in this code is binary cross-entropy, which is appropriate for binary classification tasks. If the task at hand is a multi-class classification task, a different loss function such as categorical cross-entropy should be used.\n",
    "\n",
    "\n",
    "5. The choice of the optimizer: The code uses a simple gradient descent optimizer to update the weights and biases. If the optimizer is not able to find the minimum of the loss function, the model may not converge and produce nan as the output.\n",
    "\n",
    "\n",
    "6. The choice of the dataset: The code uses a slice of the training dataset with only 1000 samples. This may not be enough to train a neural network with three hidden layers. Using a larger dataset or increasing the number of epochs may help the model to converge.\n",
    "\n",
    "\n",
    "7. To troubleshoot the issue and fix the nan output, it is recommended to check and debug each of these potential issues one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d98b8d",
   "metadata": {},
   "source": [
    "#### Try to write again the function `train_model()` properly with propper loss function. use chatGPT comment for drawing conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968186e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
